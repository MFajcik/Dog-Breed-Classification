import os
print ("LD_LIBRARY_PATH is set to \"" + str(os.environ['LD_LIBRARY_PATH']))
import numpy as np
import keras
from keras.applications.vgg19 import VGG19
from keras.models import Model
from keras.layers import Dense, Dropout, Flatten
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from tqdm import tqdm
import cv2
from sklearn.model_selection import train_test_split

#INPUT_FILES = r"/home/ifajcik/kaggle/dogbreed_data"

INPUT_FILES = r"/home/ifajcik/kaggle/dog_breed_classification/dogbreed_data"
EPOCH_CNT = 1000

df_train = pd.read_csv(INPUT_FILES+'/labels.csv')
df_test = pd.read_csv(INPUT_FILES+'/sample_submission.csv')


targets_series = pd.Series(df_train['breed'])
one_hot = pd.get_dummies(targets_series, sparse = True)

one_hot_labels = np.asarray(one_hot)
im_size = 90

x_train = []
y_train = []
x_test = []

i = 0
print ("Preprocessing training images...")
for f, breed in tqdm(df_train.values):
    img = cv2.imread(INPUT_FILES+'/train/{}.jpg'.format(f))
    label = one_hot_labels[i]
    x_train.append(cv2.resize(img, (im_size, im_size)))
    y_train.append(label)
    i += 1

# print ("Preprocessing testing images...")
# for f in tqdm(df_test['id'].values):
#     img = cv2.imread(INPUT_FILES+'/test/{}.jpg'.format(f))
#     x_test.append(cv2.resize(img, (im_size, im_size)))

y_train_raw = np.array(y_train, np.uint8)
x_train_raw = np.array(x_train, np.float32) / 255.
x_test  = np.array(x_test, np.float32) / 255.

print(x_train_raw.shape)
print(y_train_raw.shape)
print(x_test.shape)

#number of breeds
num_class = y_train_raw.shape[1]

#TODO: validation data may not contain all classes, this should be improved in future!
X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)

# Create the base pre-trained model
base_model = VGG19(weights="imagenet", include_top=False, input_shape=(im_size, im_size, 3))

# Add a new top layer
x = base_model.output

# x = Dropout(0.5)(x)
x = Flatten()(x)
x = Dense(4*num_class,activation='relu')(x)
x = Dense(2*num_class,activation='relu')(x)
predictions = Dense(num_class, activation='softmax')(x)

# This is the model we will train
model = Model(inputs=base_model.input, outputs=predictions)

# First: train only the top layers (which were randomly initialized)
for layer in base_model.layers:
    layer.trainable = False
base_model.layers[-1].trainable=True

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]
model.summary()

model.fit(X_train, Y_train, epochs=EPOCH_CNT, validation_data=(X_valid, Y_valid), verbose=1)
#
# preds = model.predict(x_test, verbose=1)
#
# sub = pd.DataFrame(preds)
# # Set column names to those generated by the one-hot encoding earlier
# col_names = one_hot.columns.values
# sub.columns = col_names
# # Insert the column id from the sample_submission at the start of the data frame
# sub.insert(0, 'id', df_test['id'])
# print(str(sub.head(5)))